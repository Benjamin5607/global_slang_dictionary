import requests
import csv
import os
import time
import random

os.makedirs("output", exist_ok=True)
OUTPUT = "output/raw_terms_reddit.csv"

# ğŸ”¥ ì—¬ê¸°ê°€ ì€ì–´ì˜ ê´‘ì‚°ì„ (ì„±ì , ë°ì´íŒ…, ê²Œì„, ë°ˆ)
SUBREDDITS = [
    "Slang",            # ì¼ë°˜ ìŠ¬ë­
    "GenZ",             # 1020ì„¸ëŒ€ ìš©ì–´
    "InternetSlang",    # ì¸í„°ë„· ìš©ì–´
    "UrbanDictionary",  # ì–´ë°˜ë”•ì…”ë„ˆë¦¬ í† ë¡ 
    "OutOfTheLoop",     # ìœ í–‰ì–´ ì§ˆë¬¸ (ì„¤ëª… êµ¿)
    "NoStupidQuestions",# ì§ˆë¬¸
    "Tinder",           # ë°ì´íŒ…/ì„±ì  ì€ì–´ (FWB, ONS ë“±)
    "ExplainLikeImFive",# ë°ˆ ì„¤ëª…
    "Twitch",           # ê²Œì„/ì¸ë°© ìš©ì–´
    "Fanfiction",       # 19ê¸ˆ/íŒ¬í”½ ìš©ì–´ (ì˜¤ë©”ê°€ë²„ìŠ¤ ë“±)
    "EnglishLearning"   # ì™¸êµ­ì¸ì´ ë¬¼ì–´ë³´ëŠ” ìŠ¬ë­
]

# ë´‡ ì°¨ë‹¨ ë°©ì§€ìš© ê°€ì§œ í—¤ë” (í•„ìˆ˜!)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

def clean_text(text):
    if not text: return ""
    return text.replace("\n", " ").replace('"', '').strip()

def fetch_reddit_data(subreddit):
    # new.json ëŒ€ì‹  hot.jsonì„ ì„ì–´ì„œ ì¸ê¸° ìˆëŠ”(ê²€ì¦ëœ) ìŠ¬ë­ ìˆ˜ì§‘
    url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit=50"
    
    try:
        res = requests.get(url, headers=HEADERS, timeout=10)
        
        # 429 Too Many Requests ë°©ì§€
        if res.status_code == 429:
            print(f"â³ Rate limited on r/{subreddit}. Waiting...")
            time.sleep(5)
            return []
            
        if res.status_code != 200:
            print(f"âš ï¸ Failed to fetch r/{subreddit}: Status {res.status_code}")
            return []
        
        data = res.json()
        posts = data.get("data", {}).get("children", [])
        extracted = []
        
        for post in posts:
            p_data = post["data"]
            
            # 1. ì œëª©(Title) ê°€ì ¸ì˜¤ê¸°
            title = clean_text(p_data.get("title", ""))
            
            # 2. ë³¸ë¬¸(Selftext) ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ì œëª©ìœ¼ë¡œ ëŒ€ì²´)
            selftext = clean_text(p_data.get("selftext", ""))
            if not selftext: 
                selftext = title
            
            # 3. ìŠ¤í‹°í‚¤(ê³µì§€ì‚¬í•­) ì œì™¸
            if p_data.get("stickied"): 
                continue

            # ğŸ”¥ í•„í„°ë§ ë¡œì§ ì™„í™”:
            # ì´ì „ì—ëŠ” "What does X mean?"ë§Œ ì°¾ì•˜ëŠ”ë°, ì´ì œëŠ” ê·¸ëƒ¥ ì œëª©ì„ ìŠ¬ë­ í›„ë³´ë¡œ ë‘¡ë‹ˆë‹¤.
            # (ë‚˜ì¤‘ì— AIê°€ ìŠ¬ë­ì¸ì§€ ì•„ë‹Œì§€ íŒë‹¨í•˜ëŠ” ê²Œ í›¨ì”¬ ì •í™•í•¨)
            
            # ë°ì´í„°ê°€ ë„ˆë¬´ ê¸¸ë©´(ì¥ë¬¸ê¸€) ë³¸ë¬¸ ì•ë¶€ë¶„ë§Œ ìë¦„
            context = f"[Title] {title} [Context] {selftext[:300]}..."

            extracted.append([
                title,   # term í›„ë³´ (ë‚˜ì¤‘ì— AIê°€ ì •ì œí•¨)
                context, # definition (ë¬¸ë§¥)
                f"Reddit (r/{subreddit})",
                "en",    # ë ˆë”§ì€ 99% ì˜ì–´ ê¸°ë°˜
                "Global"
            ])
            
        print(f"âœ… r/{subreddit}: {len(extracted)} posts collected.")
        return extracted
        
    except Exception as e:
        print(f"âŒ Error fetching r/{subreddit}: {e}")
        return []

def run():
    all_rows = []
    print("ğŸš€ Reddit Crawling Start (Deep Dive Mode)...")
    
    for sub in SUBREDDITS:
        rows = fetch_reddit_data(sub)
        all_rows.extend(rows)
        # ë´‡ íƒì§€ í”¼í•˜ê¸° ìœ„í•´ ëœë¤ ë”œë ˆì´ (1~3ì´ˆ)
        time.sleep(random.uniform(1, 3))

    with open(OUTPUT, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerow(["term", "definition", "source", "language", "country"])
        writer.writerows(all_rows)
    
    print(f"ğŸ‰ Reddit crawling finished. Total {len(all_rows)} potential terms saved.")

if __name__ == "__main__":
    run()
