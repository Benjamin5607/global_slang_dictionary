import requests
import csv
import os
import time

os.makedirs("output", exist_ok=True)
OUTPUT = "output/raw_terms_reddit.csv"

# ì€ì–´ì˜ ì„±ì§€ (Subreddits)
SUBREDDITS = [
    "Slang", "GenZ", "InternetSlang", "UrbanDictionary",
    "OutOfTheLoop", "NoStupidQuestions", "Tinder",
    "ExplainLikeImFive", "Twitch", "Fanfiction", "EnglishLearning"
]

def clean_text(text):
    if not text or text == "[deleted]" or text == "[removed]": 
        return ""
    return text.replace("\n", " ").replace('"', '').strip()

def fetch_from_pullpush(subreddit):
    # PullPush API ì—”ë“œí¬ì¸íŠ¸
    # size=50: ìµœì‹ ê¸€ 50ê°œ ê°€ì ¸ì˜¤ê¸°
    url = f"https://api.pullpush.io/reddit/search/submission/?subreddit={subreddit}&size=50&sort=desc"
    
    try:
        # ì—¬ê¸°ëŠ” ë ˆë”§ì´ ì•„ë‹ˆë¼ì„œ ê·¸ëƒ¥ requestsë¡œ ì°”ëŸ¬ë„ ë¨ (í—¤ë”ë„ í•„ìš” ì—†ìŒ)
        res = requests.get(url, timeout=20)
        
        if res.status_code != 200:
            print(f"âš ï¸ Failed to fetch r/{subreddit} via PullPush: Status {res.status_code}")
            return []
        
        data = res.json()
        posts = data.get("data", [])
        extracted = []
        
        for post in posts:
            title = clean_text(post.get("title", ""))
            selftext = clean_text(post.get("selftext", ""))
            
            # ë‚´ìš©ì´ ì—†ìœ¼ë©´ ì œëª©ë§Œ
            if not selftext: selftext = title
            
            # ì‚­ì œëœ ê¸€ ì œì™¸
            if title == "" or selftext == "": continue
            
            # ë°ì´í„° ì •ì œ
            context = f"[Title] {title} [Context] {selftext[:300]}..."

            extracted.append([
                title, 
                context, 
                f"Reddit (r/{subreddit})", 
                "en", 
                "Global"
            ])
            
        print(f"âœ… r/{subreddit}: {len(extracted)} posts collected via PullPush.")
        return extracted

    except Exception as e:
        print(f"âŒ Error scraping r/{subreddit}: {e}")
        return []

def run():
    all_rows = []
    print("ğŸš€ Reddit Crawling Start (Backdoor via PullPush)...")
    
    for sub in SUBREDDITS:
        rows = fetch_from_pullpush(sub)
        all_rows.extend(rows)
        # ì„œë²„ì— ë¶€ë‹´ ì•ˆ ì£¼ê²Œ 1ì´ˆë§Œ ì‰¬ê¸°
        time.sleep(1)

    if all_rows:
        with open(OUTPUT, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.writer(f)
            writer.writerow(["term", "definition", "source", "language", "country"])
            writer.writerows(all_rows)
        print(f"ğŸ‰ Reddit crawling finished. Total {len(all_rows)} terms saved.")
    else:
        print("âš ï¸ No data collected. PullPush might be syncing or down.")

if __name__ == "__main__":
    run()
