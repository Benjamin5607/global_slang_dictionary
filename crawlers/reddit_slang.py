import requests
import csv
import os
import re
import time

# ì €ì¥ ê²½ë¡œ
os.makedirs("output", exist_ok=True)
OUTPUT = "output/raw_terms_reddit.csv"

# íƒ€ê²Ÿ ì„œë¸Œë ˆë”§ ë¦¬ìŠ¤íŠ¸
SUBREDDITS = ["Slang", "GenZ", "InternetSlang", "UrbanDictionary"]

# ê°€ì§œ í—¤ë” (ì´ê±° ì—†ìœ¼ë©´ ë ˆë”§ì´ ì°¨ë‹¨í•¨)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

def clean_text(text):
    return text.replace("\n", " ").strip() if text else ""

def fetch_reddit_data(subreddit):
    url = f"https://www.reddit.com/r/{subreddit}/new.json?limit=50"
    try:
        res = requests.get(url, headers=HEADERS, timeout=10)
        if res.status_code != 200:
            print(f"âš ï¸ Failed to fetch r/{subreddit}: {res.status_code}")
            return []
        
        data = res.json()
        posts = data.get("data", {}).get("children", [])
        extracted = []
        
        for post in posts:
            p_data = post["data"]
            title = clean_text(p_data.get("title", ""))
            selftext = clean_text(p_data.get("selftext", ""))
            
            # ê°„ë‹¨í•œ í•„í„°ë§: ì œëª©ì´ ë„ˆë¬´ ê¸¸ë©´ ìŠ¬ë­ ë‹¨ì–´ê°€ ì•„ë‹ í™•ë¥  ë†’ìŒ
            # í˜¹ì€ "What does X mean?" íŒ¨í„´ ì¶”ì¶œ ë¡œì§ì„ ë„£ì„ ìˆ˜ë„ ìˆìŒ
            term_candidate = title
            definition_candidate = selftext
            
            # ì œëª©ì´ 'What implies...' ë˜ëŠ” 'Meaning of...' í˜•íƒœë©´ ì •ì œ (ì˜ˆì‹œ ë¡œì§)
            match = re.search(r"meaning of ['\"]?([\w\s\-]+)['\"]?", title, re.IGNORECASE)
            if match:
                term_candidate = match.group(1)

            extracted.append([
                term_candidate,
                f"[Title] {title} [Body] {definition_candidate[:200]}...", # ë¬¸ë§¥ì„ ì •ì˜ë¡œ ì €ì¥
                f"Reddit (r/{subreddit})",
                "en", # ë ˆë”§ì€ ì£¼ë¡œ ì˜ì–´
                "Global"
            ])
            
        return extracted
        
    except Exception as e:
        print(f"âŒ Error fetching r/{subreddit}: {e}")
        return []

def run():
    all_rows = []
    print("ğŸš€ Reddit Crawling Start...")
    for sub in SUBREDDITS:
        rows = fetch_reddit_data(sub)
        all_rows.extend(rows)
        time.sleep(1) # ë§¤ë„ˆ ë”œë ˆì´

    # íŒŒì¼ ì €ì¥
    with open(OUTPUT, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        # í—¤ë” ì¶”ê°€ (Dedupì—ì„œ ì½ì„ ë•Œ í—·ê°ˆë¦¬ì§€ ì•Šê²Œ)
        writer.writerow(["term", "definition", "source", "language", "country"])
        writer.writerows(all_rows)
    
    print(f"âœ… Reddit crawling finished. {len(all_rows)} terms saved.")

if __name__ == "__main__":
    run()
