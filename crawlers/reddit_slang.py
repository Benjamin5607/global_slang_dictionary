import requests
import csv
import os
import time
import random
from fake_useragent import UserAgent  # ì‹ ë¶„ ìœ„ì¡° ì „ë¬¸ê°€

os.makedirs("output", exist_ok=True)
OUTPUT = "output/raw_terms_reddit.csv"

# íƒ€ê²Ÿ ì„œë¸Œë ˆë”§
SUBREDDITS = [
    "Slang", "GenZ", "InternetSlang", "UrbanDictionary",
    "OutOfTheLoop", "NoStupidQuestions", "Tinder",
    "ExplainLikeImFive", "Twitch", "Fanfiction", "EnglishLearning"
]

def clean_text(text):
    if not text: return ""
    return text.replace("\n", " ").replace('"', '').strip()

def get_random_header():
    # ë§¤ë²ˆ ë‹¤ë¥¸ ë¸Œë¼ìš°ì €ì¸ ì²™ ìœ„ì¥
    try:
        ua = UserAgent()
        user_agent = ua.random
    except:
        # ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‹¤íŒ¨ ì‹œ ë¹„ìƒìš© í•˜ë“œì½”ë”© í—¤ë”
        user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    
    return {
        "User-Agent": user_agent,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Connection": "keep-alive"
    }

def fetch_reddit_data(subreddit):
    # www.reddit.com ëŒ€ì‹  old.reddit.comì´ë‚˜ gateway ë“±ì„ ì“¸ ìˆ˜ë„ ìˆì§€ë§Œ
    # JSON ì—”ë“œí¬ì¸íŠ¸ì— í—¤ë”ë§Œ ì˜ ì†ì´ë©´ ëš«ë¦¼
    url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit=40"
    
    headers = get_random_header()
    
    try:
        # âš ï¸ ì¤‘ìš”: ë´‡ íƒì§€ í”¼í•˜ê¸° ìœ„í•´ íƒ€ì„ì•„ì›ƒ ë„‰ë„‰íˆ
        res = requests.get(url, headers=headers, timeout=15)
        
        # 429 (Too Many Requests) -> ì ê¹ ì‰¬ì—ˆë‹¤ ê°€ê¸°
        if res.status_code == 429:
            print(f"â³ Rate limited on r/{subreddit}. Cooling down 10s...")
            time.sleep(10)
            return []

        if res.status_code != 200:
            print(f"âš ï¸ Failed to fetch r/{subreddit}: Status {res.status_code}")
            # 403ì´ ëœ¨ë©´ í•œ ë²ˆ ë” ì‹œë„ (ë‹¤ë¥¸ User-Agentë¡œ)
            if res.status_code == 403:
                print("ğŸ”„ 403 detected. Retrying with new identity...")
                time.sleep(2)
                headers = get_random_header()
                res = requests.get(url, headers=headers, timeout=15)
                if res.status_code != 200: return []
            else:
                return []
        
        data = res.json()
        posts = data.get("data", {}).get("children", [])
        extracted = []
        
        for post in posts:
            p_data = post["data"]
            
            # ìŠ¤í‹°í‚¤(ê³µì§€) ì œì™¸
            if p_data.get("stickied"): continue
            
            title = clean_text(p_data.get("title", ""))
            selftext = clean_text(p_data.get("selftext", ""))
            
            # ë³¸ë¬¸ ì—†ìœ¼ë©´ ì œëª© ì‚¬ìš©
            if not selftext: selftext = title
            
            # ë°ì´í„° ì •ì œ
            context = f"[Title] {title} [Context] {selftext[:300]}..."

            extracted.append([
                title, 
                context, 
                f"Reddit (r/{subreddit})", 
                "en", 
                "Global"
            ])
            
        print(f"âœ… r/{subreddit}: {len(extracted)} posts collected.")
        return extracted
        
    except Exception as e:
        print(f"âŒ Error fetching r/{subreddit}: {e}")
        return []

def run():
    all_rows = []
    print("ğŸš€ Reddit Crawling Start (Stealth Mode)...")
    
    for sub in SUBREDDITS:
        rows = fetch_reddit_data(sub)
        all_rows.extend(rows)
        # ë´‡ íƒì§€ í”¼í•˜ê¸° ìœ„í•´ 3~7ì´ˆ ëœë¤ ëŒ€ê¸° (ì‚¬ëŒì¸ ì²™)
        sleep_time = random.uniform(3, 7)
        time.sleep(sleep_time)

    # ë°ì´í„° ì €ì¥
    if all_rows:
        with open(OUTPUT, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.writer(f)
            writer.writerow(["term", "definition", "source", "language", "country"])
            writer.writerows(all_rows)
        print(f"ğŸ‰ Reddit crawling finished. Total {len(all_rows)} terms saved.")
    else:
        print("âš ï¸ No data collected. Reddit might be blocking aggressive requests.")

if __name__ == "__main__":
    run()
