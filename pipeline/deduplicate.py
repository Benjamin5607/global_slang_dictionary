import os
import pandas as pd
import csv

# ëª¨ë“  ì†ŒìŠ¤ íŒŒì¼ ì •ì˜
SOURCE_FILES = [
    "output/raw_terms_urban.csv",
    "output/raw_terms_wiktionary.csv",
    "output/raw_terms_reddit.csv",        # ì¶”ê°€ë¨
    "output/raw_terms_github_lists.csv"   # ì¶”ê°€ë¨
]

os.makedirs("output", exist_ok=True)

dfs = []
for path in SOURCE_FILES:
    if os.path.exists(path) and os.path.getsize(path) > 0:
        try:
            # í—¤ë”ê°€ ìˆëŠ” íŒŒì¼ë“¤ì´ë¯€ë¡œ header=0 (ê¸°ë³¸ê°’) ì‚¬ìš©
            # on_bad_lines='skip' : CSV í˜•ì‹ì´ ê¼¬ì¸ ë¼ì¸ì€ ì¿¨í•˜ê²Œ ë²„ë¦¼ (íŒŒì´í”„ë¼ì¸ ë©ˆì¶¤ ë°©ì§€)
            df = pd.read_csv(path, on_bad_lines='skip')
            
            # ì»¬ëŸ¼ ì´ë¦„ ê°•ì œ í†µì¼ (ë§Œì•½ ì†ŒìŠ¤ë§ˆë‹¤ í—¤ë”ê°€ ë‹¤ë¥´ë‹¤ë©´ ì—¬ê¸°ì„œ rename í•„ìš”)
            # í˜„ì¬ ëª¨ë“  í¬ë¡¤ëŸ¬ê°€ ["term", "definition", "source", "language", "country"] ìˆœì„œë¡œ ì €ì¥í•œë‹¤ê³  ê°€ì •
            if df.shape[1] >= 5:
                df.columns = ["term", "definition", "source", "language", "country"]
                dfs.append(df)
        except Exception as e:
            print(f"âš ï¸ Error reading {path}: {e}")
    else:
        print(f"âš ï¸ Skipping missing/empty file: {path}")

if not dfs:
    print("âŒ No data found across all sources!")
    exit(0) # ì‹¤íŒ¨ ì²˜ë¦¬ëŠ” í•˜ì§€ ë§ê³  ì¢…ë£Œ

# í†µí•©
full_df = pd.concat(dfs, ignore_index=True)

# ì •ê·œí™” (íŠ¹ìˆ˜ë¬¸ì ì œê±°, ì†Œë¬¸ì ë³€í™˜)
full_df["normalized"] = full_df["term"].astype(str).str.lower().str.replace(r"[^a-z0-9]", "", regex=True)

# ì¤‘ë³µ ì œê±° (ì •ê·œí™”ëœ ë‹¨ì–´ + ì–¸ì–´ ì¡°í•©ì´ ê°™ìœ¼ë©´ ì œê±°)
# keep='first' -> ë¨¼ì € ìˆ˜ì§‘ëœ(ë¦¬ìŠ¤íŠ¸ ì•ìª½ íŒŒì¼) ì†ŒìŠ¤ë¥¼ ìš°ì„ í•¨
# ë§Œì•½ Reddit(ìµœì‹ )ì„ ìš°ì„ í•˜ê³  ì‹¶ìœ¼ë©´ dfs ìˆœì„œë¥¼ ë°”ê¾¸ê±°ë‚˜ sort_valuesë¥¼ ì“°ë©´ ë¨
full_df = full_df.drop_duplicates(subset=["normalized", "language"], keep='first')

# ê²°ê³¼ ì €ì¥
full_df.to_csv("output/raw_terms_clean.csv", index=False, encoding="utf-8", quoting=csv.QUOTE_ALL)
print(f"ğŸ‰ Total merged terms: {len(full_df)}")
